{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro",
            "metadata": {},
            "source": [
                "# Lab 4: Support Vector Machine (SVM) Regression\n",
                "\n",
                "This notebook demonstrates the application of Support Vector Regression (SVR) with different kernel functions on the SDSS (Sloan Digital Sky Survey) dataset.\n",
                "\n",
                "## Objective\n",
                "- Implement SVR with Linear, Polynomial, and RBF kernels\n",
                "- Compare the performance of different kernels\n",
                "- Predict redshift values based on astronomical features"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports",
            "metadata": {},
            "source": [
                "## 1. Import Required Libraries\n",
                "\n",
                "First, we import all necessary libraries for data manipulation, visualization, and machine learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50300f56",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load_data",
            "metadata": {},
            "source": [
                "## 2. Load and Explore the Dataset\n",
                "\n",
                "We load the SDSS dataset which contains astronomical measurements including:\n",
                "- **objid**: Object Identifier\n",
                "- **ra, dec**: Right Ascension and Declination (coordinates)\n",
                "- **u, g, r, i, z**: Photometric measurements in different filters\n",
                "- **redshift**: Target variable (cosmological redshift)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0de39bdb",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = r\"F:\\Sem 5\\Machine Leaning\\archive (1)\\Skyserver_SQL2_27_2018 6_51_39 PM.csv\"\n",
                "df = pd.read_csv(data_path)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis\n",
                "\n",
                "Let's examine the dataset structure, check for missing values, and visualize the distribution of our target variable (redshift)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1cc4f0ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.info())\n",
                "print(df.isnull().sum())\n",
                "print(df.describe())\n",
                "\n",
                "sns.histplot(df['redshift'], bins=50, kde=True)\n",
                "plt.title(\"Distribution of Redshift (Target)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessing",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing\n",
                "\n",
                "We perform the following preprocessing steps:\n",
                "1. Remove any rows with missing values\n",
                "2. Drop unnecessary identifier columns\n",
                "3. Select relevant numeric features for modeling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0fedd87",
            "metadata": {},
            "outputs": [],
            "source": [
                "df = df.dropna()\n",
                "\n",
                "df = df.drop(columns=['objid','specobjid','plate','mjd','fiberid'])\n",
                "\n",
                "numeric_cols = ['ra','dec','u','g','r','i','z']"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "split_scale",
            "metadata": {},
            "source": [
                "## 5. Train-Test Split and Feature Scaling\n",
                "\n",
                "We split the data into training (80%) and testing (20%) sets, then standardize the features using StandardScaler.\n",
                "\n",
                "**Why scaling is important for SVM:**\n",
                "- SVM is sensitive to feature scales\n",
                "- Features with larger ranges can dominate the model\n",
                "- Standardization ensures all features contribute equally"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59c5569b",
            "metadata": {},
            "outputs": [],
            "source": [
                "X = df[numeric_cols]  \n",
                "y = df['redshift']     \n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "linear_svr",
            "metadata": {},
            "source": [
                "## 6. Linear Kernel SVR\n",
                "\n",
                "The Linear kernel is the simplest kernel function. It's suitable for linearly separable data.\n",
                "\n",
                "**Formula:** K(x, x') = x · x'\n",
                "\n",
                "**Parameters:**\n",
                "- C=1.0: Regularization parameter (controls trade-off between smooth decision boundary and classifying training points correctly)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "3ffcf546",
            "metadata": {},
            "outputs": [],
            "source": [
                "linear_svr = SVR(kernel='linear', C=1.0)\n",
                "linear_svr.fit(X_train, y_train)\n",
                "\n",
                "y_pred_linear = linear_svr.predict(X_test)\n",
                "\n",
                "print(\"Linear SVR R2 Score:\", r2_score(y_test, y_pred_linear))\n",
                "print(\"Linear SVR MSE:\", mean_squared_error(y_test, y_pred_linear))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "poly_svr",
            "metadata": {},
            "source": [
                "## 7. Polynomial Kernel SVR\n",
                "\n",
                "The Polynomial kernel can model non-linear relationships by computing polynomial combinations of features.\n",
                "\n",
                "**Formula:** K(x, x') = (γx · x' + r)^d\n",
                "\n",
                "**Parameters:**\n",
                "- degree=3: Degree of the polynomial\n",
                "- C=1.0: Regularization parameter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9baf78be",
            "metadata": {},
            "outputs": [],
            "source": [
                "poly_svr = SVR(kernel='poly', degree=3, C=1.0)\n",
                "poly_svr.fit(X_train, y_train)\n",
                "\n",
                "y_pred_poly = poly_svr.predict(X_test)\n",
                "\n",
                "print(\"Polynomial SVR R2 Score:\", r2_score(y_test, y_pred_poly))\n",
                "print(\"Polynomial SVR MSE:\", mean_squared_error(y_test, y_pred_poly))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rbf_svr",
            "metadata": {},
            "source": [
                "## 8. RBF (Radial Basis Function) Kernel SVR\n",
                "\n",
                "The RBF kernel is the most popular kernel for SVR. It can handle non-linear relationships effectively.\n",
                "\n",
                "**Formula:** K(x, x') = exp(-γ||x - x'||²)\n",
                "\n",
                "**Parameters:**\n",
                "- C=1.0: Regularization parameter\n",
                "- gamma=0.1: Kernel coefficient (controls the influence of individual training examples)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "32cf8b1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "rbf_svr = SVR(kernel='rbf', C=1.0, gamma=0.1)\n",
                "rbf_svr.fit(X_train, y_train)\n",
                "\n",
                "y_pred_rbf = rbf_svr.predict(X_test)\n",
                "\n",
                "print(\"RBF SVR R2 Score:\", r2_score(y_test, y_pred_rbf))\n",
                "print(\"RBF SVR MSE:\", mean_squared_error(y_test, y_pred_rbf))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "comparison",
            "metadata": {},
            "source": [
                "## 9. Kernel Comparison\n",
                "\n",
                "Let's visualize the R² scores of all three kernels to compare their performance.\n",
                "\n",
                "**R² Score Interpretation:**\n",
                "- R² = 1: Perfect prediction\n",
                "- R² = 0: Model performs as well as predicting the mean\n",
                "- R² < 0: Model performs worse than predicting the mean"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "4d9a8efd",
            "metadata": {},
            "outputs": [],
            "source": [
                "r2_scores = {\n",
                "    'Linear': r2_score(y_test, y_pred_linear),\n",
                "    'Polynomial': r2_score(y_test, y_pred_poly),\n",
                "    'RBF': r2_score(y_test, y_pred_rbf)\n",
                "}\n",
                "\n",
                "plt.figure(figsize=(8,5))\n",
                "sns.barplot(x=list(r2_scores.keys()), y=list(r2_scores.values()))\n",
                "plt.title(\"SVR Kernel Comparison (R2 Score)\")\n",
                "plt.ylabel(\"R2 Score\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "predictions",
            "metadata": {},
            "source": [
                "## 10. Actual vs Predicted Values Visualization\n",
                "\n",
                "This scatter plot helps us visualize how well our best model (RBF) predicts the actual values.\n",
                "\n",
                "Points closer to the diagonal line indicate better predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "d55b2974",
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(y_test, y_pred_rbf, alpha=0.5)\n",
                "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
                "plt.xlabel('Actual Redshift')\n",
                "plt.ylabel('Predicted Redshift')\n",
                "plt.title('RBF SVR: Actual vs Predicted Redshift Values')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "conclusion",
            "metadata": {},
            "source": [
                "## 11. Conclusions\n",
                "\n",
                "Based on the results:\n",
                "\n",
                "1. **RBF Kernel** performed the best with the highest R² score (~0.65)\n",
                "2. **Linear Kernel** showed moderate performance (~0.20)\n",
                "3. **Polynomial Kernel** performed poorly (negative R² score)\n",
                "\n",
                "**Key Takeaways:**\n",
                "- The relationship between features and redshift is non-linear\n",
                "- RBF kernel is most suitable for this dataset\n",
                "- Polynomial kernel may need hyperparameter tuning to improve performance\n",
                "- Feature engineering and hyperparameter optimization could further improve results"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}